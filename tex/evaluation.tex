\chapter{Evaluation}

\section{Event Detection Evaluation}
\subsection{Evaluation Data and Metric}
The evaluation for event detection uses data from IEEE AASP Challenge for event detection\footnote{http://c4dm.eecs.qmul.ac.uk/sceneseventschallenge}.  
In this challenge, a training dataset are given containing instantiations of individual events for 16 different classes. 
Moreover, there is also a developement dataset, which consist of 1 minute recordings of every-day audio events in a number of office environments. 
This developement dataset is annotated by human and can be used for model evaluation. \\ 

The evaluation begins by first segmenting clips using the a segmenter. 
We assume that only one event happens during a segment. 
Then use the trained GMM to calculate the posterior probability. 
The detected event is chosen when its corresponding GMM has the highest posterior probability.\\ 
 
Three types of assessment of the various systems will take place, i.e., a frame-based, event-based, and class-wise event-based evaluation \cite{giannoulis2013database}.  These three metrics differ mainly in the way they interpret a sequence result of detected events.  
Suppose $r$, $e$ and $c$ denotes number of ground truth, estimated and correct events, the Precision, Recall, and F-measure are defined as: 
\begin{equation}
	P = \frac{c}{e}, R = \frac{c}{r}, F = \frac{2PR}{P+R}.  
\end{equation}
For the frame-based metric, $r$, $e$, and $c$ are calculated in a 10ms window and are averaged over the entire duration. 
The event-based metric takes a different perspective of how a event unit is calculated. 
Unlike frame-based metric, which undiscriminately count event for every 10ms, this metric focuses on the true onset and offset time of ground truth events. 
Onset-only evaluation set restriction on onset time tolerance. 
Onset-offset evaluation further add a offset time restriction. 
The onset-only and onset-offset evaluation are averaged as event-based metric. 
Class-wise event-based calculate $r$, $e$, and $c$ for each class separately. And then it is averaged over classes. 
Finally, the F-measure for these three metric are averaged as the final metric of a system, and we will compare this metric with other systems. 

\subsection{Evaluation Process}
The evaluation takes on two parts: self tuning and system comparison. \\ 

Audio events are detected using the previous proposed GMM method, but there are still some parameters or configuration to be tuned for the best performance of our model.
Most importantly, the number of component number in GMM are ser by human, we run a evaluation for different component numbers from 4 to 45 to see how the number affects the F-measure. \\ 

In system comparison, we use other models from the aforementioned IEEE AASP Challenge\footnote{http://c4dm.eecs.qmul.ac.uk/sceneseventschallenge/resultsOL.html}. 
In the submitted papers, contestents also carried out the same evaluation on the developement dataset, so the comparison are done on the same dataset.  

\subsection{Evaluation Result}
\subsubsection{Component Number Evaluation}
Figure \ref{fig:component} is the result of average F-measure with regard to different component numbers in GMM. 
% component.eps
\begin{figure}[htb]
\centering
\input{figure/evaluation/component}
\caption{}
\label{fig:component}
\end{figure}

As we can see in Figure \ref{fig:component}, the F-measure gradually rise when component number are increasing. 
But after component number reaching 18 or so, the F-measure roughly stays the same. \\ 

% running time eps 
\begin{figure}[htb]
\centering
\input{figure/evaluation/componentTime}
\caption{}
\label{fig:componentTime}
\end{figure}
Moreover, Figure \ref{fig:componentTime} is the running time of our system with regard to the number of components. 
There is basically a linear relationship between the number of components and the total running time. 
Hence, we choose to use 18 as the component number for our system, since this component number has a good F-measure, and the speed for training models are also fast.\\ 

\subsubsection{Event Detection F-measure Evaluation}
\begin{table}[htb]
\centering
\caption{Event Detetion Evaluation}
\begin{tabular}{lllll}
\hline
         & EB    & CWEB  & FB    & Average \\
\hline
baseline & 15.4  & 13.5  & 20.6  & 16.50   \\
NR2      & 11.06 & 20.9  & 32.49 & 21.48   \\
NVM      & 47.53 & 41.28 & 54.41 & 47.74   \\
Our      & 53.44 & 40.07 & 52.41 & 48.64   \\
VVK      & 51.13 & 39.77 & 56.28 & 49.06   \\
GVV      & 46.8  & 36.7  & 65.2  & 49.57   \\
DHV      & 51.05 & 38.05 & 61.6  & 50.23   \\     
\hline
\end{tabular}
\end{table}
There are seven systems in the online challenge, but the system "SCS" does not list their class-wise event based result, so it is excluded for comparison. 
\begin{figure}[htb]
\centering
\input{figure/evaluation/eventdetectF1}
\caption{}
\label{fig:componentTime}
\end{figure}


\section{Scene Recognition Evaluation}
\subsection{Evaluation Data}
After we have done event detection, we use the previous proposed scene inference method to do scene recognition. 
In this section, a 10-scenes evaluation experiment are carried out for evaluating our system's performance.\\ 

The 10 scenes we used in this evaluation is \textit{bathroom, beach, concert, forest, kitchen, office, park, restaurant, street, subway station}. 
These scenes are chosen both covering indoor and outdoor, quiet and noisy environments. 
The testing clips we used are downloaded mainly from previously mentioned sound search engines, and there is 10 clips for each scene, i.e., 100 in total. \\ 

\subsection{Baseline systems}
In this evaluation, we need to compare our systems with other systems. 
And this requires getting existing audio scene recognition system with source code, since we are going to train on different scenes. 
Luckily, IEEE AASP Challenge has a scene classification part\footnote{http://c4dm.eecs.qmul.ac.uk/sceneseventschallenge/}, and source code for sume of the submitted systems\footnote{https://code.soundsoftware.ac.uk/projects/dcase2013submissions} are available for downloading. 
On the website, there are 4 scene classification systems and one baseline scene classifier. 
Because of some programming issues, we choose 2 classification systems, together with the baseline scene classifier for comparison. 
Table \ref{tab:systems} shows a summary of the systems used for comparison and our systems. 

\begin{table}[htb]
\caption{Summary of Systems}
\begin{tabular}{ll}
\hline
 Code & Method \\
\hline
 GSR & 6669 features, classifed using SVM within 4-second window, then majority voting \\
 RNH & Recurrence Quantification Analysis to MFCC time-series, classified by SVM, containing additional SVM grid search \\ 
 Baseline & MFCCs, classified with a bag-of-frames approach \\ 
 Our & MFCCs, dynamic segmentation, classified with GMM \\ 
\hline
\end{tabular}
\label{tab:systems}
\end{table}

\subsection{Evaluation Result}
Table \ref{tab:ac} show the recognition accuracy for each audio scenes. 

\begin{table}[htbp]
\centering
\resizebox{0.95\textwidth}{!}{
\begin{tabular}{lcccccccccc}
\toprule
     & bathroom & beach & concert & forest & kitchen & office & park & restaurant & street & subway station \\
\bottomrule
top1 & $20\%$ & {\color{blue}$70\%$} & $10\%$ & $30\%$ & $20\%$ & $10\%$ & $60\%$ & $20\%$ & $100\%$ & $30\%$ \\
top3 & $40\%$ & $80\%$ & $40\%$ & $90\%$ & $80\%$ & $30\%$ & $80\%$ & $100\%$ & $100\%$ & $70\%$ \\
\bottomrule
\end{tabular}
}
\caption{Recognition Accuracy for 10 Audio Scenes}
\label{tab:ac}
\end{table}


\section{Summary}
