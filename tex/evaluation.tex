\chapter{Evaluation}
Experiments of the performance of our system is evaluated here. 
We evaluate our event detection and scene recognition system respectively. 
For both event and scene evaluation, we have a evaluation on self model tuning, and another evaluation for system comparison. 

\section{Event Detection Evaluation}
\subsection{Evaluation Data and Metric}
The evaluation for event detection uses data from IEEE AASP Challenge for event detection\footnote{http://c4dm.eecs.qmul.ac.uk/sceneseventschallenge}.  
In this challenge, a training dataset are given containing instantiations of individual events for 16 different classes. 
Moreover, there is also a developement dataset, which consist of 1 minute recordings of every-day audio events in a number of office environments. 
This developement dataset is annotated by human and can be used for model evaluation. \\ 

The evaluation begins by first segmenting clips using the a segmenter. 
We assume that only one event happens during a segment. 
Then use the trained GMM to calculate the posterior probability. 
The detected event is chosen when its corresponding GMM has the highest posterior probability.\\ 
 
Three types of assessment of the various systems will take place, i.e., a frame-based, event-based, and class-wise event-based evaluation \cite{giannoulis2013database}.  These three metrics differ mainly in the way they interpret a sequence result of detected events.  
Suppose $r$, $e$ and $c$ denotes number of ground truth, estimated and correct events, the Precision, Recall, and F-measure are defined as: 
\begin{equation}
	P = \frac{c}{e}, R = \frac{c}{r}, F = \frac{2PR}{P+R}.  
\end{equation}
For the frame-based metric, $r$, $e$, and $c$ are calculated in a 10ms window and are averaged over the entire duration. 
The event-based metric takes a different perspective of how a event unit is calculated. 
Unlike frame-based metric, which undiscriminately count event for every 10ms, this metric focuses on the true onset and offset time of ground truth events. 
Onset-only evaluation set restriction on onset time tolerance. 
Onset-offset evaluation further add a offset time restriction. 
The onset-only and onset-offset evaluation are averaged as event-based metric. 
Class-wise event-based calculate $r$, $e$, and $c$ for each class separately. And then it is averaged over classes. 
Finally, the F-measure for these three metric are averaged as the final metric of a system, and we will compare this metric with other systems. 

\subsection{Evaluation Result}

\subsubsection{Self Tuning}
Audio events are detected using the previous proposed GMM method, but there are still some parameters or configuration to be tuned for the best performance of our model.
Most importantly, the number of component number in GMM are ser by human, we run a evaluation for different component numbers from 4 to 45 to see how the number affects the F-measure. 
Figure \ref{fig:component} is the result of average F-measure with regard to different component numbers in GMM. 
% component.eps
\begin{figure}[htb!]
\centering
\input{figure/evaluation/component}
\caption{F-measure for different component number}
\label{fig:component}
\end{figure}

As we can see in Figure \ref{fig:component}, the F-measure gradually rise when component number are increasing. 
But after component number reaching 18 or so, the F-measure roughly stays the same.  

% running time eps 
\begin{figure}[htb!]
\centering
\input{figure/evaluation/componentTime}
\caption{Running time for different componet number}
\label{fig:componentTime}
\end{figure}
Moreover, Figure \ref{fig:componentTime} is the running time of our system with regard to the number of components. 
There is basically a linear relationship between the number of components and the total running time. 
Hence, we choose to use 18 as the component number for our system, since this component number has a good F-measure, and the speed for training models are also fast.

\subsubsection{System Comparison}
In system comparison, we use other models from the aforementioned IEEE AASP Challenge\footnote{http://c4dm.eecs.qmul.ac.uk/sceneseventschallenge/resultsOL.html}. 
In the submitted papers, contestents also carried out the same evaluation on the developement dataset, so the comparison are done on the same dataset.  
\begin{table}[htb]
\centering
\caption{Event Detetion Evaluation}
\begin{tabular}{lllll}
\hline
         & EB    & CWEB  & FB    & Average \\
\hline
baseline & 0.154  & 0.135  & 0.206  & 0.1650   \\
NR2      & 0.1106 & 0.209  & 0.3249 & 0.2148   \\
NVM      & 0.4753 & 0.4128 & 0.5441 & 0.4774   \\
Our      & 0.5344 & 0.4007 & 0.5241 & 0.4864   \\
VVK      & 0.5113 & 0.3977 & 0.5628 & 0.4906   \\
GVV      & 0.468  & 0.367  & 0.652  & 0.4957   \\
DHV      & 0.5105 & 0.3805 & 0.616  & 0.5023   \\     
\hline
\end{tabular}
\end{table}
There are seven systems in the online challenge, but the system "SCS" does not list their class-wise event based result, so it is excluded for comparison. 
\begin{figure}[htb!]
\centering
\input{figure/evaluation/eventdetectF1}
\caption{Event Detection F-Measure}
\label{fig:eventdetectF1}
\end{figure}
As we could see visually in Figure \ref{fig:eventdetectF1}, our event detection system have a accuracy close to the highest-performing system. 


\section{Scene Recognition Evaluation}

\subsection{Evaluation Data}
After we have done event detection, we use the previous proposed scene inference method to do scene recognition. 
In this section, a 10-scenes evaluation experiment are carried out for evaluating our system's performance.

The 10 scenes we used in this evaluation is \textit{bathroom, beach, concert, forest, kitchen, office, park, restaurant, street, subway station}. 
These scenes are chosen both covering indoor and outdoor, quiet and noisy environments. 
The testing clips we used are downloaded mainly from previously mentioned sound search engines, and there is 10 clips for each scene, i.e., 100 in total. 

\subsection{Baseline systems}
In this evaluation, we need to compare our systems with other systems. 
And this requires getting existing audio scene recognition system with source code, since we are going to train on different scenes. 
Luckily, IEEE AASP Challenge has a scene classification part\footnote{\url{http://c4dm.eecs.qmul.ac.uk/sceneseventschallenge/}}, and source code for sume of the submitted systems\footnote{\url{https://code.soundsoftware.ac.uk/projects/dcase2013submissions}} are available for downloading. 
On the website, there are 4 scene classification systems and one baseline scene classifier. 
Because of some programming issues, we choose 2 classification systems, together with the baseline scene classifier for comparison. 
Table \ref{table:systems} shows a summary of the systems used for comparison and our systems. 

\begin{table}[htb!]
\caption{Summary of systems}
\begin{tabular}{ll}
\toprule
 Code & Method \\
\bottomrule
 Baseline & MFCCs, classified with a bag-of-frames approach \\ 
 RNH & Recurrence Quantification Analysis to MFCC time-series, classified by SVM \\ 
 GSR & 6669 features, classifed using SVM within 4-second window, then majority voting \\
 Our & MFCCs, dynamic segmentation, classified with GMM \\ 
\bottomrule
\end{tabular}
\label{table:systems}
\end{table}

\subsection{Evaluation Result}

\subsubsection{Self Tuning}
In the self tuning part of our scene recognition system. 
We conducted a evaluation on how noise reduction can affect the scene recognition accuracy. 
In the training process, the clips could be denoised before training GMMs, and the same reason applies to the testing process. 
So here are two factors that affect the event detection result: whether training data are denoised, and whether testing data are denoised. 
We denote the model name for these different condition in the format of \textit{out\_trainDenoise\_testDenoise}. 
The last two variable would be represented by $0$ and $1$. 
For example, \textit{our\_1\_1} represent the model which has the training data and testing data denoised.  

\begin{table}[htb!]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccccccc}
\toprule
     & bathroom & beach & concert & forest & kitchen & office & park & restaurant & street & subway station & Average \\
\bottomrule
our\_0\_0 & $70\%$ & $20\%$ & $30\%$ & $60\%$ & $50\%$ & $70\%$ & $10\%$ & $0\%$  & $90\%$ & $0\%$  & $40\%$\\
our\_0\_1 & $60\%$ & $60\%$ & $50\%$ & $50\%$ & $50\%$ & $70\%$ & $50\%$ & $0\%$  & $60\%$ & $0\%$  & $45\%$\\
our\_1\_0 & $60\%$ & $0\%$  & $40\%$ & $20\%$ & $40\%$ & $80\%$ & $10\%$ & $0\%$  & $90\%$ & $0\%$  & $34\%$\\
our\_1\_1 & $80\%$ & $60\%$ & $60\%$ & $50\%$ & $60\%$ & $80\%$ & $40\%$ & $10\%$ & $100\%$& $30\%$ & $57\%$\\
\bottomrule
\end{tabular}
}
\caption{Recognition accuracy for our different models}
\label{table:ourTuning}
\end{table}

Table \ref{table:ourTuning} shows the detailed result of this experiment. 
As expected, for different training process, denoising the testing data always bring an improvement in accuracy. 
Moreover, denoising the training further boost the result. 

\begin{figure}[htb!]
\centering
\input{figure/evaluation/ourTuning}
\caption{Recognition accuracy for our different models}
\label{fig:ourTuning}
\end{figure}

Figure \ref{fig:ourTuning} is a graphical present of the data in Table \ref{table:ourTuning}. 
In the x-axis label, ``Train on denoised'' means that we are training on denoised data, and the other are trained on undenoised data. 
For one group, there is two bar representing given that training condition, two testing experiments are done. 
The blue bar is the result of testing on undenoised data, and the brown bar is the result of testing on denoised data. 
It is clearly shown in the figure that denoising testing data always improves the accuracy. 
Our highest performance is achieved at $57\%$ when we train our model on denoised data and also test on denoised data. 

\subsubsection{System Comparison}
Table \ref{table:ac} show the recognition accuracy for each audio scenes. 
As we could see in this table, GSR has a highest average accuracy of $61\%$, while our model comes second with accuracy of $54\%$. 
However, the variation in our recognition result is very large. 
Our model have 5 scenes which get the highest accuracy, but at the same time, perform bad in \textit{restaurant} and \textit{subway station}. 
This is reasonable for our approach. 
Because our system work by detecting distinct audible events. 
Thus favoring scenes where many distinct events exist, like in office, \textit{phone, printer, paper} could be a distinct event. 
But in scene like \textit{restaurant}, it is generally filled with noise atmosphere, thus hard for our system to detect something useful. 

\begin{table}[htb!]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccccccc}
\toprule
     & bathroom & beach & concert & forest & kitchen & office & park & restaurant & street & subway station & Average \\
\bottomrule
Baseline & $60\%$ & $40\%$ & $60\%$ & $40\%$ & $10\%$ & $40\%$ & \textbf{$60\%$} & $30\%$ & $60\%$ & $70\%$ & $47\%$\\
RNH1 & $30\%$ & $40\%$ & $70\%$ & $40\%$ & $20\%$ & $30\%$ & $50\%$ & $50\%$ & $70\%$ & $50\%$ & $45\%$ \\
RNH2 & $20\%$ & $40\%$ & $70\%$ & $40\%$ & $40\%$ & $30\%$ & $50\%$ & $40\%$ & $70\%$ & $60\%$ & $46\%$ \\
GSR & $60\%$ & $50\%$ & \textbf{$90\%$} & \textbf{$50\%$} & $30\%$ & $60\%$ & \textbf{$60\%$} & \textbf{$60\%$} & $70\%$ &\textbf{ $80\%$} & \textbf{$61\%$} \\
Our & \textbf{$80\%$} & \textbf{$60\%$} & $60\%$ & \textbf{$50\%$} & \textbf{$60\%$} & \textbf{$80\%$} & $40\%$ & $10\%$ & \textbf{$100\%$} & $30\%$ & $57\%$ \\
\bottomrule
\end{tabular}
}
\caption{Recognition accuracy for 10 audio scenes}
\label{table:ac}
\end{table}

\begin{figure}[htb!]
\centering
\input{figure/evaluation/sceneeval}
\caption{Recognition accuracy for 10 audio scenes}
\label{fig:sceneeval}
\end{figure}

In the table \ref{table:ac}, we could see that GSR has the highest average accuracy which reaches to $61\%$. 
And our system ranked the second with $57\%$. 
Yet, our system has the highest accuracy in 6 different scenes, and even reaches a perfect accuracy in scene \textit{street}. 

\begin{figure}[htb!]
\centering
\input{figure/evaluation/ourgsr}
\caption{Comparison of Our's with GSR}
\label{fig:ourgsr}
\end{figure}

Looking closely at the Figure \ref{fig:ourgsr}, we could find that our system performs bad in \textit{restaurant} and \textit{subway station}, only $10\%$ and $30\%$ respectively. 
We think that this is because the testing data of these two scenes are generally noisy. 
Our system work in a way to detect audible events our an audio clip. 
But when the clip is noisy, it is hard for our system to detect any audible events embedded in it, which is the case in \textit{restaurant} and \textit{subway station}. 


\cleardoublepage{}
\section{Summary}
In this chapter, we introduced the evaluation of our systems. 
It include two parts: first the event detection evaluation and then the scene recognition evaluation.

In the first evaluation, we compare our model with submitted systems to the IEEE AASP Challenge for event detection. 
This evaluation uses 16 audible events, and each event has 20 clips for training. 
The testing datas are three long audio, where each may contain 10 to 20 events. 
The task is to build a model trained on training data and detect out the events in the testing data, where the onset and offset time are also required. 
Our system build GMMs on MFCC features, and segment the testing clips with a dynamic segmentor. 
In the evaluation, we has a average F-measure of 0.4864, which come close to the highest one: 0.5023. 

In the scene recognition evaluation, we first conduct an experiment on how noise reduction affects the performance of our system. 
The result is that noise reduction would boost the performance, and we have the highest accuracy when both training data and testing data are denoised. 
So, we use this mechanism in the following evaluation. 
In the system comparison part, we compare the performance of our system with other 4 algorithms in a 10 scene classification problem. 
The 10 scene are chosen to be the common audio scene seen in everyday life. 
The result is that we achieved second in the average accuracy. 
But it is notable that we have six scenes with accuracies ranked the first.  
